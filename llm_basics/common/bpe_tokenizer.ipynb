{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cc2fae5-e2c7-4c9b-9fe7-9b77b3773b26",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding (BPE) Tokenizer\n",
    "\n",
    "BPE是一种常用的子词(subword)分词算法，被广泛应用于现代语言模型(如GPT系列)中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a078e5a-0289-4dd1-8116-f021f94bb9cd",
   "metadata": {},
   "source": [
    "## 统计相邻token频率\n",
    "\n",
    "- 功能：统计token序列中所有相邻token对的出现频率\n",
    "- 示例输入：`[1, 2, 3, 1, 2]`\n",
    "- 输出：`{(1, 2): 2, (2, 3): 1, (3, 1): 1}` \n",
    "- 说明：token对(1,2)出现2次，其他各出现1次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5332ec5-0544-4d4c-9a02-682bcd4815c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token id array:\n",
      "[1, 2, 3, 1, 2]\n",
      "get stats:\n",
      "{(1, 2): 2, (2, 3): 1, (3, 1): 1}\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids, counts=None):\n",
    "    counts = {} if counts is None else counts\n",
    "    for pair in zip(ids, ids[1:]): # 遍历连续的token对\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "example = [1, 2, 3, 1, 2] # token id 序列\n",
    "counts = get_stats(example)\n",
    "print('token id array:')\n",
    "print(example)\n",
    "print('get stats:')\n",
    "print(counts) # 相邻token出现频次"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3473b83-8cc6-4551-a2e3-5442c47085cb",
   "metadata": {},
   "source": [
    "## 合并token对\n",
    "\n",
    "- 功能：将序列中所有指定的token对替换为一个新token\n",
    "- 示例：将`[1, 2, 3, 1, 2]`中的(1,2)替换为4\n",
    "- 输出：`[4, 3, 4]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea8d7610-cad1-4623-ade3-b92fcae0e9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)  # 匹配到pair则替换为新token\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "ids=[1, 2, 3, 1, 2]\n",
    "pair=(1, 2)\n",
    "# 在 ids 中用 pair 匹配，匹配到替换为新 token id 4\n",
    "newids = merge(ids, pair, 4)\n",
    "print(newids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f33b80-0562-451b-b42d-7910bfae17a2",
   "metadata": {},
   "source": [
    "## BPE Tokenizer实现\n",
    "\n",
    "### 1 编码阶段（构建词汇表）\n",
    "**输入**：原始文本 + 预设词汇表大小  \n",
    "**输出**：包含常见子词的词汇表\n",
    "\n",
    "**步骤**：\n",
    "1. **初始化**：\n",
    "   - 将文本拆分为最小单元（如ASCII字符或字节）\n",
    "   - 初始词汇表=所有基础字符\n",
    "\n",
    "2. **迭代合并**：\n",
    "   - **统计频率**：计算所有相邻字节对的出现频率\n",
    "   - **合并最高频对**：将最高频的字节对合并为新符号\n",
    "   - **更新词汇表**：将新符号加入词汇表\n",
    "   - **替换文本**：用新符号替换所有该字节对的出现\n",
    "\n",
    "3. **终止条件**：\n",
    "   - 达到预设词汇表大小\n",
    "   - 或无可合并的字节对（所有频次=1）\n",
    "\n",
    "### 2 解码阶段\n",
    "\n",
    "逆向操作：从最高ID开始，逐步将合并符号替换回原始字节对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaa39b2c-61a5-462c-aea9-2bc0d05f1b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_VOCAB_SIZE = 256\n",
    "\n",
    "class BasicTokenizer():\n",
    "    def __init__(self):\n",
    "        self.merges = {}  # 存储合并规则：(token1, token2) -> new_token\n",
    "        self.vocab = self.build_vocab()  # token_id到字节的映射\n",
    "        \n",
    "    def build_vocab(self):\n",
    "        # 初始词表包含所有单字节(0-255)\n",
    "        vocab = {idx: bytes([idx]) for idx in range(INITIAL_VOCAB_SIZE)}\n",
    "        # 添加合并后的token\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            # bytes 加法等价于字符串拼接\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "        return vocab\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        assert vocab_size >= INITIAL_VOCAB_SIZE\n",
    "        num_merges = vocab_size - INITIAL_VOCAB_SIZE\n",
    "\n",
    "        text_bytes = text.encode(\"utf-8\") \n",
    "        ids = list(text_bytes)  # 初始化为字节级token\n",
    "\n",
    "        merges = {} \n",
    "        # int -> bytes (初始词表，直接 idx 到字节映射)\n",
    "        vocab = {idx: bytes([idx]) for idx in range(INITIAL_VOCAB_SIZE)}\n",
    "\n",
    "        for i in range(num_merges):\n",
    "            stats = get_stats(ids)  # 统计相邻token频率\n",
    "            pair = max(stats, key=stats.get)  # 选择最频繁的token对\n",
    "            new_idx = INITIAL_VOCAB_SIZE + i  # 新token的id从256开始分配\n",
    "            ids = merge(ids, pair, new_idx)  # 合并token对\n",
    "            merges[pair] = new_idx  # 记录合并规则\n",
    "            # 原来的词不会剔除，而是在基础词表上增加\n",
    "            vocab[new_idx] = vocab[pair[0]] + vocab[pair[1]]  # 更新词表\n",
    "\n",
    "        self.merges = merges # used in encode()\n",
    "        self.vocab = vocab   # used in decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76eb8f8c-9cf4-413c-a9be-a2e6c15afc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534\n",
      "b's '\n",
      "b'e '\n",
      "b' t'\n",
      "b'at'\n",
      "b'in'\n",
      "b' th'\n",
      "b'an'\n",
      "b'.\\n'\n",
      "b'li'\n",
      "b've'\n",
      "{(115, 32): 256, (101, 32): 257, (32, 116): 258, (97, 116): 259, (105, 110): 260, (258, 104): 261, (97, 110): 262, (46, 10): 263, (108, 105): 264, (118, 101): 265}\n"
     ]
    }
   ],
   "source": [
    "text = '''   \n",
    "Cats never fail to fascinate human beings.\n",
    "They can be friendly and affectionate towards humans, but they lead mysterious lives of their own as well.\n",
    "They never become submissive like dogs and horses. As a result, humans have learned to respect feline independence.\n",
    "Most cats remain suspicious of humans all their lives.\n",
    "One of the things that fascinates us most about cats is the popular belief that they have nine lives.\n",
    "Apparently, there is a good deal of truth in this idea. A cat's ability to survive falls is based on fact.\n",
    "'''\n",
    "\n",
    "text_bytes = text.encode(\"utf-8\") # raw bytes\n",
    "ids = list(text_bytes) # list of integers in range 0..255\n",
    "print(len(ids))\n",
    "\n",
    "bpe = BasicTokenizer()\n",
    "bpe.train(text, vocab_size=266)\n",
    "for i in range(256 , 266, 1):\n",
    "    print(bpe.vocab[i])\n",
    "\n",
    "print(bpe.merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b03f752-470c-4482-bb0c-463b7376f3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair:\n",
      "(101, 32)\n",
      "b'e' b' '\n",
      "pair:\n",
      "(97, 116)\n",
      "b'a' b't'\n",
      "pair:\n",
      "(108, 105)\n",
      "b'l' b'i'\n",
      "pair:\n",
      "(100, 111)\n",
      "b'd' b'o'\n",
      "16\n",
      "13\n",
      "[100, 111, 32, 121, 111, 117, 32, 264, 107, 257, 99, 259, 115]\n"
     ]
    }
   ],
   "source": [
    "# Encode\n",
    "# utf-8 token ids\n",
    "text = 'do you like cats'\n",
    "text_bytes = text.encode(\"utf-8\") # raw bytes\n",
    "# 首先对数据转成字符的 token id\n",
    "# 再将 raw token id 按照 merges 表对 raw token id 进行合并 -> token_id\n",
    "\n",
    "# bpe token ids\n",
    "ids = list(text_bytes) # list of integers in range 0..255\n",
    "while len(ids) >= 2:\n",
    "    stats = get_stats(ids)\n",
    "    # 结果取min: merge对应idx越小，出现的频率越高\n",
    "    pair = min(stats, key=lambda p: bpe.merges.get(p, float(\"inf\"))) \n",
    "    print('pair:')\n",
    "    print(pair)\n",
    "    print(bpe.vocab[pair[0]], bpe.vocab[pair[1]])\n",
    "    if pair not in bpe.merges:\n",
    "        break \n",
    "    idx = bpe.merges[pair] # (3,4) -> 268\n",
    "    ids = merge(ids, pair, idx) # (2,3,4,5) -> (2, 268, 5)\n",
    "print(len(text))\n",
    "print(len(ids))\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbe0d023-7c2e-4dca-a6ab-30df7d6036d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do you like cats\n"
     ]
    }
   ],
   "source": [
    "# Decode\n",
    "text_bytes = b\"\".join(bpe.vocab[idx] for idx in ids)\n",
    "decode_text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "print(decode_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db98c18e-e3de-4812-8db7-11893f44cac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
