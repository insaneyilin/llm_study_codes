{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c01bf01-885d-435a-8798-ecd035d939e5",
   "metadata": {},
   "source": [
    "# Grouped Query Attention\n",
    "\n",
    "Grouped Query Attention (GQA) 是 Transformer 架构中一种注意力机制的变体，介于多头注意力(MHA)和多查询注意力(MQA)之间。在 GQA 中：\n",
    "- 查询头(Q)的数量保持与标准多头注意力相同\n",
    "- 键头(K)和值头(V)的数量减少，并被多个查询头共享\n",
    "- 这种设计可以在保持较好模型性能的同时减少内存带宽需求\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84b4b6b9-407e-4dad-80a6-74bac03e4b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "import math\n",
    "\n",
    "class ModelArgs:\n",
    "    dim: int = 18               # 嵌入词向量维度为18\n",
    "    n_layers: int = 1           # 1层Transformer\n",
    "    n_heads: int = 6            # 6个查询头(Q)\n",
    "    n_kv_heads: int = 2         # 2个键值头(KV)\n",
    "    vocab_size: int = -1\n",
    "    multiple_of: int = 10\n",
    "    norm_eps: float = 1e-5\n",
    "    rope_theta: float = 500000\n",
    "    max_batch_size: int = 2\n",
    "    max_seq_len: int = 17\n",
    "    model_parallel_size = 1      # 默认为1(单GPU)\n",
    "\n",
    "# 每个头的维度：head_dim = dim / n_heads = 18/6 = 3\n",
    "\n",
    "# Q头的总维度：n_heads * head_dim = 6*3 = 18\n",
    "\n",
    "# KV头的总维度：n_kv_heads * head_dim = 2*3 = 6\n",
    "\n",
    "# 每个KV头被复制的次数：n_rep = n_heads / n_kv_heads = 6/2 = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4c9e9d6-365c-418b-bffb-77192f38375e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 6, 3])\n",
      "tensor([[-0.3614,  1.3549,  0.1143],\n",
      "        [-0.3614,  1.3549,  0.1143],\n",
      "        [-0.3614,  1.3549,  0.1143],\n",
      "        [ 0.9436, -0.4128,  0.4709],\n",
      "        [ 0.9436, -0.4128,  0.4709],\n",
      "        [ 0.9436, -0.4128,  0.4709]])\n"
     ]
    }
   ],
   "source": [
    "# KV头复制函数\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"将KV头复制n_rep次以匹配Q头的数量\"\"\"\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "\n",
    "# 示例\n",
    "k = torch.randn(1, 7, 2, 3)  # batch=1, seqlen=7, n_kv_heads=2, head_dim=3\n",
    "repeat_k = repeat_kv(k, 3)   # 将每个KV头复制3次\n",
    "print(repeat_k.shape)        # torch.Size([1, 7, 6, 3])\n",
    "print(repeat_k[0,0,:,:])     # 可以看到每个KV头被复制了3次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "975c6df4-d66a-4b61-9afc-a9881ce5817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_kv_heads = args.n_kv_heads\n",
    "        self.model_parallel_size = args.model_parallel_size\n",
    "        self.n_local_heads = args.n_heads // self.model_parallel_size\n",
    "        self.n_local_kv_heads = self.n_kv_heads // self.model_parallel_size\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        # 线性变换层\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "\n",
    "        # KV缓存(用于推理)\n",
    "        self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len, \n",
    "                                  self.n_local_kv_heads, self.head_dim))\n",
    "        self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len, \n",
    "                                  self.n_local_kv_heads, self.head_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, \n",
    "               freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        \n",
    "        # 计算Q,K,V\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "\n",
    "        # 更新KV缓存\n",
    "        self.cache_k[:bsz, start_pos:start_pos+seqlen] = xk\n",
    "        self.cache_v[:bsz, start_pos:start_pos+seqlen] = xv\n",
    "        keys = self.cache_k[:bsz, :start_pos+seqlen]\n",
    "        values = self.cache_v[:bsz, :start_pos+seqlen]\n",
    "\n",
    "        # 复制KV头以匹配Q头数量\n",
    "        keys = repeat_kv(keys, self.n_rep)\n",
    "        values = repeat_kv(values, self.n_rep)\n",
    "\n",
    "        # 调整维度顺序并计算注意力\n",
    "        xq = xq.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores += mask\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values)\n",
    "\n",
    "        # 合并多头输出\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "        return self.wo(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bd64060-8852-48fa-9ea3-585666542392",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.attention = Attention(args)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, start_pos: int, \n",
    "               freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        return x + self.attention(x, start_pos, freqs_cis, mask)  # 残差连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a91e134b-4d75-4a84-820c-1105c2d9f514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个GPU上的Q头数: 3\n",
      "每个GPU上的KV头数: 1\n"
     ]
    }
   ],
   "source": [
    "config = ModelArgs()\n",
    "config.model_parallel_size = 2  # 假设使用2个GPU\n",
    "\n",
    "attn_parallel = Attention(config)\n",
    "print(f'每个GPU上的Q头数: {attn_parallel.n_local_heads}')  # 3\n",
    "print(f'每个GPU上的KV头数: {attn_parallel.n_local_kv_heads}')  # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29532f99-ee09-4331-ad12-eb3dfdc67645",
   "metadata": {},
   "outputs": [],
   "source": [
    "分配策略：\n",
    "```\n",
    "GPU0: Q1, Q2, Q3, K1, V1\n",
    "GPU1: Q4, Q5, Q6, K2, V2\n",
    "```\n",
    "\n",
    "计算过程：\n",
    "1. 每个GPU获取自己负责的Q头和KV头\n",
    "2. GPU0将K1,V1复制3次，GPU1将K2,V2复制3次\n",
    "3. 各自计算注意力：\n",
    "   - GPU0: Q1,Q2,Q3 与 K1(复制3份),V1(复制3份) 计算\n",
    "   - GPU1: Q4,Q5,Q6 与 K2(复制3份),V2(复制3份) 计算\n",
    "4. 合并结果：GPU1将输出发送到GPU0，GPU0将所有输出拼接并通过Wo线性层\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4583d668-b217-4424-87ba-907b9fe16026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([1, 7, 18])\n",
      "输出形状: torch.Size([1, 7, 18])\n"
     ]
    }
   ],
   "source": [
    "# 初始化配置和模型\n",
    "config = ModelArgs()\n",
    "llama_block = TransformerBlock(1, config)\n",
    "\n",
    "# 创建输入 (batch=1, seqlen=7, dim=18)\n",
    "x_src = torch.randn(1, 7, 18)\n",
    "\n",
    "# 前向传播\n",
    "y = llama_block(x_src, start_pos=0, freqs_cis=None, mask=None)\n",
    "\n",
    "print(f\"输入形状: {x_src.shape}\")  # torch.Size([1, 7, 18])\n",
    "print(f\"输出形状: {y.shape}\")      # torch.Size([1, 7, 18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2450ebd3-4694-4b87-9b56-96db6b21f81f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
