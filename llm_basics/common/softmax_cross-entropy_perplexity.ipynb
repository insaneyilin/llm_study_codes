{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f73d1e2-2dce-47d2-849e-e5585d07710b",
   "metadata": {},
   "source": [
    "# Cross Entropy\n",
    "\n",
    "设事件 $X$ 发生的概率为 $P(X)$，从直觉出发，概率越小，事件发生的“惊喜”程度越大，惊喜程度可以用 $\\frac{1}{P(x)}$ 衡量。\n",
    "\n",
    "取对数，则惊喜程度可以定义为：\n",
    "\n",
    "$$\n",
    "\\log(\\frac{1}{P(x)}) = -\\log(P(x))\n",
    "$$\n",
    "\n",
    "确定性事件（概率为1）的惊喜度为 0；对于不同独立事件，上式定义的惊喜度可以增加。\n",
    "\n",
    "信息熵：真实概率分布下，惊喜程度的期望。\n",
    "\n",
    "$$\n",
    "H(P) = -\\int_{x} P(x) \\log(P(x)) \\ dx\n",
    "$$\n",
    "\n",
    "交叉熵：考虑真实概率分布 $P$ 和预测概率分布 $Q$，预测事件的惊喜程度在真实概率分布下的期望即为交叉熵：\n",
    "\n",
    "$$\n",
    "H(P, Q) = -\\int_{x} P(x) \\log(Q(x)) \\ dx\n",
    "$$\n",
    "\n",
    "KL 散度，交叉熵 - 真实概率分布信息熵：\n",
    "\n",
    "$$\n",
    "KL(P, Q) = H(P, Q) - H(P) = -\\int_{x} P(x) \\log(\\frac{P(x)}{Q(x)}) \\ dx\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c90012-48a0-4374-a0b7-ff384773f1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "x = torch.arange(0, 100) / 100.0  # probabiliy [0, 1]\n",
    "plt.plot(x.tolist(), (-x.log()).tolist(), '-g', label=\"-log(x)\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('-log(x)')  # “surpriseness”\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a473f365-313f-4a52-83e6-552a9383cf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2408, 0.6414, 0.1204])\n",
      "tensor(1.0026)\n"
     ]
    }
   ],
   "source": [
    "# 计算两个离散概率分布的交叉熵\n",
    "p = torch.tensor([0.2, 0.7, 0.1])  # 真实概率分布\n",
    "q = torch.tensor([0.3, 0.4, 0.3])  # 预测概率分布\n",
    "ce = -p * q.log()\n",
    "print(ce)\n",
    "print(ce.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0980701c-594f-4832-9cad-5aea2d6d55d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3219, 0.3576, 0.1609])\n",
      "tensor(0.8404)\n"
     ]
    }
   ],
   "source": [
    "# 假设预测分布接近真实分布\n",
    "q = torch.tensor([0.2, 0.6, 0.2])  # 预测概率分布\n",
    "ce = -p * q.log()\n",
    "print(ce)\n",
    "print(ce.sum())\n",
    "# 交叉熵减小了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4fe98d0-7cbf-4a21-a47e-b0b1b5066e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.9163, 0.0000])\n",
      "tensor(0.9163)\n",
      "tensor([1.2040, 0.9163, 1.2040])\n"
     ]
    }
   ],
   "source": [
    "# 考虑真实分布是 one-hot 标签形式\n",
    "p = torch.tensor([0.0, 1.0, 0.0])  # 真实概率分布, one-hot 标签\n",
    "q = torch.tensor([0.3, 0.4, 0.3])  # 预测概率分布\n",
    "ce = -p * q.log()\n",
    "print(ce)\n",
    "print(ce.sum())\n",
    "print(-q.log())\n",
    "# 可以看到，直接取真实标签在 -log(q) 上对应的值就是交叉熵的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14d11557-8f4d-4f14-98ac-194513f77827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9178)\n",
      "tensor(0.9178)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_ce_loss(logits, labels):\n",
    "    \"\"\"\n",
    "    计算交叉熵损失\n",
    "    logits: (batch_size, num_cls)\n",
    "    labels: (batch_size, )\n",
    "    \"\"\"\n",
    "    # 确保labels是torch.long类型\n",
    "    labels = labels.long()\n",
    "\n",
    "    # 计算负对数似然 (negative log likelihood)\n",
    "    # log_softmax 数值更稳定，直接算 softmax 可能会导致数值溢出\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # 从 log_probs 中提取真实标签对应的对数概率\n",
    "    nll_loss = -log_probs.gather(dim=-1, index=labels.unsqueeze(-1)).squeeze(-1)\n",
    "    # 等价于:\n",
    "    # batch_size = labels.shape[0]\n",
    "    # idx = torch.arange(0, batch_size)\n",
    "    # nll_loss = -log_probs[idx, labels]\n",
    "\n",
    "    # 对整个 batch 取平均\n",
    "    return nll_loss.mean()\n",
    "\n",
    "logits = torch.tensor([[0.2, 0.8], [0.6, 0.4]])\n",
    "labels = torch.tensor([0, 1])\n",
    "\n",
    "ce = calculate_ce_loss(logits, labels)\n",
    "print(ce)\n",
    "ce_loss_torch = torch.nn.CrossEntropyLoss()(logits, labels)\n",
    "print(ce_loss_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca7b0e2-31fe-4e35-a50b-a72de429c962",
   "metadata": {},
   "source": [
    "# Softmax\n",
    "\n",
    "把 logits 向量归一化到概率分布向量\n",
    "\n",
    "$$\n",
    "Softmax(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{d}e^{x_j}}\n",
    "$$\n",
    "\n",
    "## 减常数 C 避免数值溢出\n",
    "\n",
    "exp() 容易指数爆炸溢出，一般可以先将 logits 向量减去一个常量，比如 logits 中的最大值:\n",
    "\n",
    "$$\n",
    "Softmax(x_i) = \\frac{e^{x_i - C}}{\\sum_{j=1}^{d}e^{x_j - C}}\n",
    "$$\n",
    "\n",
    "## Log Softmax\n",
    "\n",
    "很多时候我们都需要对 logits 计算 log softmax 的值，但注意 log 函数在 0 点会取到 -inf ，导致数据溢出，我们可以直接计算 log softmax 值：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "log(Softmax(x_i)) &= log(\\frac{e^{x_i - C}}{\\sum_{j=1}^{d}e^{x_j - C}}) \\\\\n",
    "&= (x_i - C) - log(\\sum_{j=1}^{d}e^{x_j - C})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "此时 $log(\\sum_{j=1}^{d}e^{x_j - C})$ 不会溢出，这种方式计算的 log softmax (log probs) 也不会溢出。\n",
    "\n",
    "我们可以通过 softmax 把 log probs 转回 probs :\n",
    "\n",
    "$$\n",
    "Softmax(x_i) = \\frac{e^{log \\ probs (i)}}{\\sum_{j=1}^{d}e^{log \\ probs (j)}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfed8fe4-5f60-4314-b5eb-4330b72c29e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1., 0.]])\n",
      "Directly computed log softmax(i.e. lob probs): \n",
      "tensor([[-inf, -inf, 0., -inf]])\n",
      "Log softmax log probs:\n",
      "tensor([[-9990., -9998.,     0., -9996.]])\n",
      "recovered_probs:\n",
      "tensor([[0., 0., 1., 0.]])\n",
      "tensor([[0., 0., 1., 0.]])\n",
      "torch.nn.functional.log_softmax(logits): \n",
      "tensor([[-9990., -9998.,     0., -9996.]])\n"
     ]
    }
   ],
   "source": [
    "def safe_softmax(logits):\n",
    "    # logits shape [batch_size, dim]\n",
    "    logits_max, _ = logits.max(dim=-1)\n",
    "    logits = logits - logits_max.unsqueeze(1) \n",
    "    logits = logits.exp()\n",
    "    logits_sum = logits.sum(-1, keepdim = True)\n",
    "    prob = logits / logits_sum\n",
    "    return prob.abs()\n",
    "\n",
    "\n",
    "def log_softmax(logits, recover_prob=False):\n",
    "    logits_max, _ = logits.max(dim=-1)\n",
    "    safe_logits = logits - logits_max.unsqueeze(1)\n",
    "    exp_logits = safe_logits.exp()\n",
    "    exp_logits_sum = exp_logits.sum(-1, keepdim=True)\n",
    "    log_probs = (logits - logits_max.unsqueeze(1)) - exp_logits_sum.log()\n",
    "\n",
    "    if recover_prob is True:\n",
    "        exp_log_probs = log_probs.exp()\n",
    "        probs = exp_log_probs / exp_log_probs.sum(-1, keepdim=True)\n",
    "        return probs, log_probs\n",
    "\n",
    "    return log_probs\n",
    "\n",
    "logits = torch.tensor([[10, 2, 10000, 4]], dtype=torch.float32)\n",
    "probs = safe_softmax(logits)\n",
    "print(probs)\n",
    "print('Directly computed log softmax(i.e. lob probs): ')\n",
    "print(probs.log())\n",
    "\n",
    "recovered_probs, log_probs = log_softmax(logits, recover_prob=True)\n",
    "print('Log softmax log probs:')\n",
    "print(log_probs)\n",
    "print('recovered_probs:')\n",
    "print(recovered_probs)\n",
    "print(safe_softmax(log_probs))\n",
    "\n",
    "print('torch.nn.functional.log_softmax(logits): ')\n",
    "print(torch.nn.functional.log_softmax(logits, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4858988-e4ec-459b-8707-84f612b460e7",
   "metadata": {},
   "source": [
    "# Perplexity Metric of Language Model\n",
    "\n",
    "语言模型对文本的困惑度（Perplexity），即模型对真实文本的“惊讶程度”，越低表示文本越流畅（公式：`exp(NLL)`，NLL为负对数似然）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "281c8de7-7e51-4944-ad32-77c01b9a4a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6897, 0.9398, 0.0486]]) tensor([0.5594])\n",
      "tensor([[ 20.0000,   3.8652, 997.0000]]) tensor([340.2884])\n",
      "tensor(1.7496)\n",
      "tensor(5.1847e+21)\n"
     ]
    }
   ],
   "source": [
    "def CE_loss(logits, labels):\n",
    "    \"\"\"\n",
    "    计算语言模型 next token prediction 输出的交叉熵损失\n",
    "    logits: (batch_size, seq_len, vocab_size)\n",
    "    labels: (batch_size, seq_len)\n",
    "    返回:\n",
    "    - token级平均损失: (batch_size, seq_len)\n",
    "    - sequence级平均损失: (batch_size,)\n",
    "    \"\"\"\n",
    "    # 计算负对数似然 (negative log likelihood)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)  # 先计算log softmax\n",
    "    nll_loss = -log_probs.gather(dim=-1, index=labels.long().unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    # nll_loss, average token loss, shape (batch_size, seq_len)\n",
    "\n",
    "    # 计算每个sequence的平均损失\n",
    "    seq_loss = nll_loss.mean(dim=-1)  # (batch_size, )\n",
    "\n",
    "    return nll_loss, seq_loss\n",
    "\n",
    "\n",
    "def calculate_perplexity(loss):\n",
    "    \"\"\"\n",
    "    从交叉熵损失计算困惑度\n",
    "    loss: 每个 seq 的交叉熵损失值, (batch_size,)\n",
    "    \"\"\"\n",
    "    # batch 维度取 mean\n",
    "    max_loss = 50  # avoid inf\n",
    "    clipped_loss = torch.clamp(loss, max=max_loss)\n",
    "    return clipped_loss.exp().mean()\n",
    "\n",
    "labels = torch.tensor([[0, 1, 2]], dtype=torch.float32)\n",
    "\n",
    "# logits1 is more accurate\n",
    "logits1 = torch.tensor([[[0.8, 0.1, 0.1],\n",
    "                         [0.3, 0.5, 0.2],\n",
    "                         [-100, 0, 3]]], dtype=torch.float32)\n",
    "\n",
    "logits2 = torch.tensor([[[0.0, 20, 0.1],\n",
    "                         [1.2, 0.5, 4.3],\n",
    "                         [1000, 0, 3]]], dtype=torch.float32)\n",
    "\n",
    "nll_loss1, seq_loss1 = CE_loss(logits1, labels)\n",
    "nll_loss2, seq_loss2 = CE_loss(logits2, labels)\n",
    "print(nll_loss1, seq_loss1)\n",
    "print(nll_loss2, seq_loss2)\n",
    "\n",
    "ppl1 = calculate_perplexity(seq_loss1)\n",
    "ppl2 = calculate_perplexity(seq_loss2)\n",
    "print(ppl1)  # logits1 has lower ppl\n",
    "print(ppl2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b10342-218b-4053-b892-706a603ff8c8",
   "metadata": {},
   "source": [
    "# Softmax Cross Entropy 求导\n",
    "\n",
    "对于多分类问题，Softmax函数将模型输出转换为概率分布：\n",
    "\n",
    "$$\n",
    "y_i = \\frac{e^{z_i}}{\\sum_{j=1}^C e^{z_j}} \\quad \\text{其中} \\quad i = 1,2,...,C\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $C$ 是类别总数\n",
    "- $z_i$ 是第 i 个类别的logit值（softmax层的输入）\n",
    "- $y_i$ 是第 i 个类别的预测概率\n",
    "\n",
    "Softmax的导数分为两种情况 (用复合函数求导法则推一下，h(x) = f(x)/g(x))：\n",
    "\n",
    "1. 当对同一位置求导时（i = j）：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial z_j} = y_i(1 - y_j)\n",
    "$$\n",
    "\n",
    "2. 当对不同位置求导时（i ≠ j）：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial z_j} = -y_i y_j\n",
    "$$\n",
    "\n",
    "矩阵形式表示:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{z}} = \\text{diag}(\\mathbf{y}) - \\mathbf{y}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "\n",
    "交叉熵损失函数定义\n",
    "\n",
    "给定：\n",
    "- 真实标签的one-hot编码 $\\mathbf{p} = [p_1, p_2, ..., p_C]$\n",
    "- 预测概率分布 $\\mathbf{q} = [q_1, q_2, ..., q_C]$\n",
    "\n",
    "交叉熵损失为：\n",
    "\n",
    "$$\n",
    "H(\\mathbf{p}, \\mathbf{q}) = -\\sum_{i=1}^C p_i \\log q_i\n",
    "$$\n",
    "\n",
    "交叉熵对softmax输出的导数\n",
    "\n",
    "$$\n",
    "\\frac{\\partial H}{\\partial q_i} = -\\frac{p_i}{q_i}\n",
    "$$\n",
    "\n",
    "## Softmax + Cross-Entropy 联合求导\n",
    "\n",
    "\n",
    "前向计算路径:\n",
    "\n",
    "$$\n",
    "\\text{logits} \\ (z) \\xrightarrow{\\text{softmax}} \\text{probs} \\ (q) \\xrightarrow{\\text{CE}} \\text{loss} \\ (H)\n",
    "$$\n",
    "\n",
    "反向求导步骤:\n",
    "\n",
    "根据链式法则：\n",
    "$$\n",
    "\\frac{\\partial H}{\\partial z_j} = \\sum_{i=1}^C \\frac{\\partial H}{\\partial q_i} \\cdot \\frac{\\partial q_i}{\\partial z_j}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $\\frac{\\partial H}{\\partial q_i} = -\\frac{p_i}{q_i}$ （交叉熵对softmax输出的导数）\n",
    "- $\\frac{\\partial q_i}{\\partial z_j}$ 是softmax对logits的Jacobian矩阵\n",
    "\n",
    "展开计算 \n",
    "\n",
    "将两部分导数代入：\n",
    "$$\n",
    "\\frac{\\partial H}{\\partial z_j} = \\underbrace{\\left(-\\frac{p_j}{q_j}\\right) \\cdot q_j(1-q_j)}_{\\text{当 } i=j} + \\sum_{i \\neq j} \\underbrace{\\left(-\\frac{p_i}{q_i}\\right) \\cdot (-q_i q_j)}_{\\text{当 } i \\neq j}\n",
    "$$\n",
    "\n",
    "化简后：\n",
    "$$\n",
    "= -p_j (1-q_j) + \\sum_{i \\neq j} p_i q_j = q_j - p_j\n",
    "$$\n",
    "\n",
    "即 softmax cross entropy 直接对 logits 求导，得到了非常简单的形式\n",
    "\n",
    "\n",
    "**为什么能直接跳过中间步骤？**\n",
    "\n",
    "1. 数学巧合\n",
    "   交叉熵的梯度 $\\frac{\\partial H}{\\partial q_i} = -\\frac{p_i}{q_i}$ 与 softmax 的 Jacobian 矩阵相乘后，所有中间项完美抵消，最终得到极其简洁的形式 $q_j - p_j$。\n",
    "2. 物理意义\n",
    "   梯度直接反映了预测概率 $q_j$ 与真实标签 $p_j$ 的差异，这与直觉一致：\n",
    "   - 当 $q_j > p_j$（预测过度自信）：梯度为正，需要降低该logit\n",
    "   - 当 $q_j < p_j$（预测不足）：梯度为负，需要增加该logit\n",
    "\n",
    "**工程实现中的注意事项**\n",
    "\n",
    "不要分开实现\n",
    "\n",
    "```python\n",
    "# 错误做法（数值不稳定且低效）\n",
    "q = softmax(z)\n",
    "loss = cross_entropy(q, p)\n",
    "grad = ...  # 手动链式求导\n",
    "```\n",
    "\n",
    "应该直接使用联合函数\n",
    "```python\n",
    "# PyTorch（推荐）\n",
    "loss = F.cross_entropy(z, targets)  # z是logits，targets是类别索引\n",
    "\n",
    "# TensorFlow\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=p, logits=z)\n",
    "```\n",
    "\n",
    "这些函数内部：\n",
    " - 自动处理softmax的数值稳定性（如减去最大值）\n",
    " - 直接计算联合梯度 $q_j - p_j$，避免中间矩阵运算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1cb954b-d8c4-4228-b397-e38bb7d649af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# 随机生成logits和标签\n",
    "logits = torch.randn(1, 5, requires_grad=True)  # batch_size=1, num_classes=5\n",
    "labels = torch.tensor([2])  # 第3个类别（从0开始）\n",
    "\n",
    "# 框架自动计算\n",
    "loss = F.cross_entropy(logits, labels)\n",
    "loss.backward()\n",
    "auto_grad = logits.grad.clone()\n",
    "\n",
    "# 手动计算\n",
    "probs = F.softmax(logits, dim=1)\n",
    "target_probs = torch.zeros_like(probs)\n",
    "target_probs[0, labels.item()] = 1\n",
    "manual_grad = probs - target_probs\n",
    "\n",
    "# 比较结果\n",
    "print(torch.allclose(auto_grad, manual_grad))  # 应输出True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b1f329-e89b-4dd5-8376-b9aef2ce56f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
