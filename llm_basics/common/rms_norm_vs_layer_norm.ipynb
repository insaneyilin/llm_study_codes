{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4199f2b9-bd34-42fc-b25a-5af05ec49a9d",
   "metadata": {},
   "source": [
    "RMSNorm 去中心化(即不使用β参数)的影响:\n",
    "1. 减少了模型参数，降低了计算复杂度\n",
    "2. 可能提高模型的泛化能力，因为减少了与特定任务相关的先验假设\n",
    "3. 在T5等模型中的成功应用表明，对于某些任务，中心化操作可能不是必要的\n",
    "\n",
    "1. **LayerNorm**:\n",
    "   - 同时进行中心化(减去均值)和标准化(除以标准差)\n",
    "   - 包含两个可学习参数γ和β\n",
    "   - 计算成本略高于RMSNorm\n",
    "\n",
    "2. **RMSNorm**:\n",
    "   - 仅进行标准化(除以均方根)\n",
    "   - 通常只使用一个可学习参数γ\n",
    "   - 具有尺度不变性，梯度与输入尺度成反比\n",
    "   - 计算效率更高，在某些任务中表现与LayerNorm相当\n",
    "\n",
    "3. **选择建议**:\n",
    "   - 当输入分布已经接近零均值时，RMSNorm可能是更高效的选择\n",
    "   - 对于需要强归一化的情况，LayerNorm可能更合适\n",
    "   - 实际应用中可以通过实验确定哪种归一化更适合特定任务\n",
    "\n",
    "在计算资源受限或模型很大时优先考虑 RMSNorm\n",
    "\n",
    "当不确定哪种更好时，可以在验证集上进行小规模实验\n",
    "\n",
    "一些现代架构如 LLAMA 已经采用 RMSNorm 替代 LayerNorm\n",
    "\n",
    "两者可以混合使用，如关键层使用 LayerNorm，其他层使用 RMSNorm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28266096-25c3-40b0-8b60-307641780308",
   "metadata": {},
   "source": [
    "RMSNorm具有尺度不变性，即输入乘以一个标量，输出结果不变(除了缩放参数γ的影响)。\n",
    "\n",
    "RMSNorm的梯度与输入尺度成反比，这是其数值稳定性的重要特性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c163698-3ae5-4a6f-bed9-37428e014b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))  # 可学习的缩放参数\n",
    "\n",
    "    def _norm(self, x: torch.Tensor):\n",
    "        # x: (batch, seq_len, dim)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.weight * self._norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25f3a8a9-0b8b-4661-9687-f7ecb022d116",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))  # 缩放参数\n",
    "        self.bias = nn.Parameter(torch.zeros(dim))    # 偏置参数\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, unbiased=False, keepdim=True)\n",
    "        return self.weight * (x - mean) / torch.sqrt(var + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "107c608f-9deb-47a6-b31f-bafb65824e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([2, 5, 8])\n",
      "RMSNorm 输出形状: torch.Size([2, 5, 8])\n",
      "LayerNorm 输出形状: torch.Size([2, 5, 8])\n",
      "\n",
      "尺度不变性测试:\n",
      "RMSNorm 原始输出与缩放输出比例: 1.0000005960464478\n",
      "LayerNorm 原始输出与缩放输出比例: 1.0000005960464478\n"
     ]
    }
   ],
   "source": [
    "# 测试数据\n",
    "batch, seq_len, dim = 2, 5, 8\n",
    "x = torch.randn(batch, seq_len, dim)\n",
    "\n",
    "# 初始化两种归一化\n",
    "rms_norm = RMSNorm(dim)\n",
    "ln_norm = LayerNorm(dim)\n",
    "\n",
    "# 前向传播\n",
    "rms_out = rms_norm(x)\n",
    "ln_out = ln_norm(x)\n",
    "\n",
    "print(\"输入形状:\", x.shape)\n",
    "print(\"RMSNorm 输出形状:\", rms_out.shape)\n",
    "print(\"LayerNorm 输出形状:\", ln_out.shape)\n",
    "\n",
    "# 尺度不变性测试\n",
    "x_scaled = x * 10.0\n",
    "rms_scaled = rms_norm(x_scaled)\n",
    "ln_scaled = ln_norm(x_scaled)\n",
    "\n",
    "print(\"\\n尺度不变性测试:\")\n",
    "print(\"RMSNorm 原始输出与缩放输出比例:\", (rms_scaled / rms_out).mean().item())\n",
    "print(\"LayerNorm 原始输出与缩放输出比例:\", (ln_scaled / ln_out).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cc94531-c55f-4947-8da8-2c4728b51142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "计算效率对比 (1000次前向传播):\n",
      "RMSNorm: 0.7783 秒\n",
      "LayerNorm: 1.1759 秒\n",
      "速度提升: 33.8%\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "# 计时测试\n",
    "def time_normalization(norm, x):\n",
    "    def fn():\n",
    "        return norm(x)\n",
    "    return timeit.timeit(fn, number=100000)\n",
    "\n",
    "rms_time = time_normalization(rms_norm, x)\n",
    "ln_time = time_normalization(ln_norm, x)\n",
    "\n",
    "print(\"\\n计算效率对比 (1000次前向传播):\")\n",
    "print(f\"RMSNorm: {rms_time:.4f} 秒\")\n",
    "print(f\"LayerNorm: {ln_time:.4f} 秒\")\n",
    "print(f\"速度提升: {(ln_time - rms_time)/ln_time:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e0cc789-9c83-4662-bdb8-dbecd1ed4398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "梯度行为对比:\n",
      "RMSNorm 梯度均值: 0.004218118265271187\n",
      "LayerNorm 梯度均值: -2.3283064365386963e-10\n",
      "RMSNorm 梯度方差: 0.0006827886682003736\n",
      "LayerNorm 梯度方差: 0.0006636029575020075\n"
     ]
    }
   ],
   "source": [
    "# 梯度测试\n",
    "x.requires_grad_(True)\n",
    "target = torch.randn_like(x)\n",
    "\n",
    "# RMSNorm 梯度\n",
    "rms_out = rms_norm(x)\n",
    "rms_loss = (rms_out - target).pow(2).mean()\n",
    "rms_loss.backward()\n",
    "rms_grad = x.grad.clone()\n",
    "x.grad.zero_()\n",
    "\n",
    "# LayerNorm 梯度\n",
    "ln_out = ln_norm(x)\n",
    "ln_loss = (ln_out - target).pow(2).mean()\n",
    "ln_loss.backward()\n",
    "ln_grad = x.grad.clone()\n",
    "\n",
    "print(\"\\n梯度行为对比:\")\n",
    "print(\"RMSNorm 梯度均值:\", rms_grad.mean().item())\n",
    "print(\"LayerNorm 梯度均值:\", ln_grad.mean().item())\n",
    "print(\"RMSNorm 梯度方差:\", rms_grad.var().item())\n",
    "print(\"LayerNorm 梯度方差:\", ln_grad.var().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a00d524-0547-4da2-88ac-ec7a3b5d14bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
